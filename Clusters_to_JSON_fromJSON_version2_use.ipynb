{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bb12db-009a-452f-8885-340b3019a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc21751-499e-43e1-9d8f-94aebe55aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6e5605-1a28-476e-b29d-213eff515190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1310563-cbcf-47ef-8754-ef6b4e4c19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c38d57-5443-4ca5-90bd-c7ef9aa697f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # view all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8624e99b-7e7a-4bc7-a52b-a1357a8d000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Extract.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ad6b8a-cdaa-46d4-91c3-14cb1e13ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go into data directory\n",
    "cur_dir = os.getcwd()\n",
    "data_dir = r\"C:\\Users\\jnunez\\Documents\\Projects\\SFR B2R Community Clustering\\Data\\DBSCAN\"\n",
    "\n",
    "# import data from data file\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876cd0b4-b58b-4e34-b056-de7f653a7856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All_Core_Points (ignore).csv',\n",
       " \"Clus_Match_clean (with Louis's - ignore - not a good metric).csv\",\n",
       " 'Clus_Match_unclean.csv',\n",
       " 'Louis_Master_Lists_Merged_raw.csv',\n",
       " 'Master_List_Clusters.csv',\n",
       " 'Super_List_Clusters.csv',\n",
       " 'b2r_master_list_all_cluster_evaluations.csv',\n",
       " 'b2r_master_list_final.csv',\n",
       " 'b2r_super_list_final.csv',\n",
       " 'failed_b2r_master_list.csv',\n",
       " 'failed_b2r_super_list.csv',\n",
       " 'file_name.txt',\n",
       " 'non-site_json1.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view files\n",
    "files = sorted(os.listdir(data_dir))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ea6adf-f6d4-4f3f-820b-95956928319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_list = pd.read_csv('b2r_super_list_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76026ae-4d54-4c5c-b646-c2b54aebd84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Owner Name</th>\n",
       "      <th>OwnerID</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>BlockFIPS</th>\n",
       "      <th>FIPSCode</th>\n",
       "      <th>StateFips</th>\n",
       "      <th>CLUSTERS_DBSCAN</th>\n",
       "      <th>Unique_Cluster</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332012856</td>\n",
       "      <td>AMH - 6043 Theodore Street</td>\n",
       "      <td>6043 Theodore Street</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89081.0</td>\n",
       "      <td>AMH4R</td>\n",
       "      <td>3</td>\n",
       "      <td>36.270469</td>\n",
       "      <td>-115.120230</td>\n",
       "      <td>3.200300e+14</td>\n",
       "      <td>32003.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NV_3_0</td>\n",
       "      <td>964266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>303684489</td>\n",
       "      <td>AMH - 2006 Reaves Way</td>\n",
       "      <td>2006 Reaves Way</td>\n",
       "      <td>North Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89081.0</td>\n",
       "      <td>American Homes 4 Rent - Las Vegas</td>\n",
       "      <td>3</td>\n",
       "      <td>36.269628</td>\n",
       "      <td>-115.121217</td>\n",
       "      <td>3.200300e+14</td>\n",
       "      <td>32003.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NV_3_0</td>\n",
       "      <td>964266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         UID                        Name               Address  \\\n",
       "0  332012856  AMH - 6043 Theodore Street  6043 Theodore Street   \n",
       "1  303684489       AMH - 2006 Reaves Way       2006 Reaves Way   \n",
       "\n",
       "              City State      Zip                         Owner Name  OwnerID  \\\n",
       "0        Las Vegas    NV  89081.0                              AMH4R        3   \n",
       "1  North Las Vegas    NV  89081.0  American Homes 4 Rent - Las Vegas        3   \n",
       "\n",
       "    Latitude   Longitude     BlockFIPS  FIPSCode  StateFips  CLUSTERS_DBSCAN  \\\n",
       "0  36.270469 -115.120230  3.200300e+14   32003.0       32.0              0.0   \n",
       "1  36.269628 -115.121217  3.200300e+14   32003.0       32.0              0.0   \n",
       "\n",
       "  Unique_Cluster    hash  \n",
       "0         NV_3_0  964266  \n",
       "1         NV_3_0  964266  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_list.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e0eadb-8299-435b-a781-915a30c57e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>OwnerID</th>\n",
       "      <th>CLUSTERS_DBSCAN</th>\n",
       "      <th>Num_Homes</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>City</th>\n",
       "      <th>Unique_Cluster</th>\n",
       "      <th>B2R_final_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NV</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>36.270673</td>\n",
       "      <td>-115.120495</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV_3_0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NV</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>36.256843</td>\n",
       "      <td>-115.122496</td>\n",
       "      <td>North Las Vegas</td>\n",
       "      <td>NV_3_1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NV</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>36.236204</td>\n",
       "      <td>-115.227261</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV_3_2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NV</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>36.049038</td>\n",
       "      <td>-114.980929</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>NV_3_3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NV</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>36.041480</td>\n",
       "      <td>-115.213039</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV_3_4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>AR</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>36.046932</td>\n",
       "      <td>-94.180920</td>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>AR_17_1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>OR</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>45.513564</td>\n",
       "      <td>-122.851695</td>\n",
       "      <td>Beaverton</td>\n",
       "      <td>OR_3_0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>LA</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>30.408540</td>\n",
       "      <td>-91.010535</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>LA_17_0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>LA</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>30.451468</td>\n",
       "      <td>-91.187147</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>LA_17_1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>IA</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>41.686358</td>\n",
       "      <td>-93.762984</td>\n",
       "      <td>Johnston</td>\n",
       "      <td>IA_17_0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>542 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    State  OwnerID  CLUSTERS_DBSCAN  Num_Homes   Latitude   Longitude  \\\n",
       "0      NV        3                0        166  36.270673 -115.120495   \n",
       "1      NV        3                1         74  36.256843 -115.122496   \n",
       "2      NV        3                2         13  36.236204 -115.227261   \n",
       "3      NV        3                3        134  36.049038 -114.980929   \n",
       "4      NV        3                4         16  36.041480 -115.213039   \n",
       "..    ...      ...              ...        ...        ...         ...   \n",
       "537    AR       17                1         13  36.046932  -94.180920   \n",
       "538    OR        3                0         17  45.513564 -122.851695   \n",
       "539    LA       17                0         38  30.408540  -91.010535   \n",
       "540    LA       17                1         16  30.451468  -91.187147   \n",
       "541    IA       17                0         17  41.686358  -93.762984   \n",
       "\n",
       "                City Unique_Cluster  B2R_final_check  \n",
       "0          Las Vegas         NV_3_0             True  \n",
       "1    North Las Vegas         NV_3_1             True  \n",
       "2          Las Vegas         NV_3_2             True  \n",
       "3          Henderson         NV_3_3             True  \n",
       "4          Las Vegas         NV_3_4             True  \n",
       "..               ...            ...              ...  \n",
       "537     Fayetteville        AR_17_1             True  \n",
       "538        Beaverton         OR_3_0             True  \n",
       "539      Baton Rouge        LA_17_0             True  \n",
       "540      Baton Rouge        LA_17_1             True  \n",
       "541         Johnston        IA_17_0             True  \n",
       "\n",
       "[542 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_list = pd.read_csv('b2r_master_list_final.csv')\n",
    "master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1499b00-bae8-4c54-ad73-f04e0da2c2be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AL_sfr.json',\n",
       " 'AR_sfr.json',\n",
       " 'AZ_sfr.json',\n",
       " 'CA_sfr.json',\n",
       " 'CO_sfr.json',\n",
       " 'CT_sfr.json',\n",
       " 'DC_sfr.json',\n",
       " 'DE_sfr.json',\n",
       " 'FL_sfr.json',\n",
       " 'GA_sfr.json',\n",
       " 'HI_sfr.json',\n",
       " 'IA_sfr.json',\n",
       " 'ID_sfr.json',\n",
       " 'IL_sfr.json',\n",
       " 'IN_sfr.json',\n",
       " 'KS_sfr.json',\n",
       " 'KY_sfr.json',\n",
       " 'LA_sfr.json',\n",
       " 'MA_sfr.json',\n",
       " 'MD_sfr.json',\n",
       " 'MI_sfr.json',\n",
       " 'MN_sfr.json',\n",
       " 'MO_sfr.json',\n",
       " 'MS_sfr.json',\n",
       " 'MT_sfr.json',\n",
       " 'NC_sfr.json',\n",
       " 'ND_sfr.json',\n",
       " 'NE_sfr.json',\n",
       " 'NJ_sfr.json',\n",
       " 'NM_sfr.json',\n",
       " 'NV_sfr.json',\n",
       " 'NY_sfr.json',\n",
       " 'OH_sfr.json',\n",
       " 'OK_sfr.json',\n",
       " 'OR_sfr.json',\n",
       " 'PA_sfr.json',\n",
       " 'RI_sfr.json',\n",
       " 'SC_sfr.json',\n",
       " 'SD_sfr.json',\n",
       " 'TN_sfr.json',\n",
       " 'TX_sfr.json',\n",
       " 'UT_sfr.json',\n",
       " 'VA_sfr.json',\n",
       " 'WA_sfr.json',\n",
       " 'WI_sfr.json',\n",
       " 'WV_sfr.json',\n",
       " 'WY_sfr.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory for importation\n",
    "states_dir = r\"T:\\Single Family\\SFR\\02. DATABASE\\JSON\"\n",
    "os.chdir(states_dir)\n",
    "\n",
    "dir_files = os.listdir()\n",
    "\n",
    "# get state data\n",
    "dir_files = [file for file in dir_files if \".json\" in file]\n",
    "dir_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11f9a6-7a28-45c6-83e4-5a7cf31c12e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38644d3d-e9ef-4c01-9161-f463d10114d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL\n",
      "AR\n",
      "AZ\n",
      "CA\n",
      "CO\n",
      "CT\n",
      "DC\n",
      "DE\n",
      "FL\n",
      "GA\n",
      "HI\n",
      "IA\n",
      "ID\n",
      "IL\n",
      "IN\n",
      "KS\n",
      "KY\n",
      "LA\n",
      "MA\n",
      "MD\n",
      "MI\n",
      "MN\n",
      "MO\n",
      "MS\n",
      "MT\n",
      "NC\n",
      "ND\n",
      "NE\n",
      "NJ\n",
      "NM\n",
      "NV\n",
      "NY\n",
      "OH\n",
      "OK\n",
      "OR\n",
      "PA\n",
      "RI\n",
      "SC\n",
      "SD\n",
      "TN\n",
      "TX\n",
      "UT\n",
      "VA\n",
      "WA\n",
      "WI\n",
      "WV\n",
      "WY\n"
     ]
    }
   ],
   "source": [
    "for state_file_name in dir_files:\n",
    "    os.chdir(states_dir)\n",
    "    clean_by_state(state_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f3b699b-22cd-45e5-90a1-118f951d02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates and merges different data sets to have the \n",
    "# json file format we want\n",
    "\n",
    "def clean_by_state(state_file_name):\n",
    "    state = state_file_name[:2]\n",
    "    print(state)\n",
    "\n",
    "    # read file\n",
    "    with open(state_file_name, 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "\n",
    "    # parse file\n",
    "    obj = json.loads(data)\n",
    "\n",
    "    df = pd.DataFrame(obj)\n",
    "\n",
    "    '''\n",
    "    parsed_data = defaultdict(list)\n",
    "\n",
    "    for entry in obj:\n",
    "        recursive_parser(entry, parsed_data, \"\")\n",
    "\n",
    "    d = dict(parsed_data)\n",
    "\n",
    "    # dictionary to data frame\n",
    "    ddf = pd.DataFrame(d)\n",
    "    '''\n",
    "\n",
    "    to_drop = ['UBID', 'AVMPrice', 'AdditionalFees',  'OwnerID', 'Vintage', \n",
    "               # 'AddressStreet', 'AddressCity', 'AddressState', 'AddressZip', \n",
    "               'Homebuilder']\n",
    "\n",
    "    sfr = df.drop(to_drop, axis = 1)\n",
    "\n",
    "    sfr = sfr.rename({'BCPID': 'UID', \n",
    "                      'AddressStreet': 'Street',\n",
    "                      'AddressCity': 'City',\n",
    "                      'AddressState': 'State',\n",
    "                      'AddressZip': 'Zip'\n",
    "                     }, axis =1)\n",
    "\n",
    "    st_super = super_list.loc[super_list['State'] == state]\n",
    "\n",
    "\n",
    "    # remove duplicates\n",
    "\n",
    "    sfr['missing'] = [len(x) if type(x) == list else 0 for x in df['Rent']]\n",
    "\n",
    "\n",
    "    sfr['City'] = sfr['City'].str.upper()\n",
    "\n",
    "    dup_same_city_idx = sfr[sfr.duplicated(['UID', 'City'],keep=False)].sort_values(by='UID').index\n",
    "\n",
    "    same_city = [1 if x in dup_same_city_idx else 0 for x in range(0, sfr.shape[0])]\n",
    "\n",
    "    # same city\n",
    "\n",
    "    sfr['dup_same_city'] = same_city\n",
    "    last_column = sfr.pop('dup_same_city')\n",
    "    sfr.insert(1, 'dup_same_city', last_column)\n",
    "    last_column = sfr.pop('missing')\n",
    "    sfr.insert(1, 'missing', last_column)\n",
    "    dups = sfr[sfr.duplicated(['UID'],keep=False)].sort_values(by=['UID','dup_same_city'])\n",
    "\n",
    "    # get ones with least amount of missing vals and that doesn't have a differing city from other dups\n",
    "    not_same_city_idx = dups[dups['dup_same_city'] == 0].index \n",
    "    # drop\n",
    "    sfr1 = sfr.drop(not_same_city_idx)\n",
    "\n",
    "    # from dups with same cities, leave the one with the most information\n",
    "    sfr1 = sfr1.sort_values(by=['UID','missing'])\n",
    "\n",
    "\n",
    "    sfr2 =sfr1.drop_duplicates(['UID'],keep='last') # keep one with the least missing information, if same city\n",
    "\n",
    "\n",
    "    # merge sfr and superlist using uid\n",
    "    full = st_super.merge(sfr2,on='UID')\n",
    "\n",
    "\n",
    "    # b2r had a very small amount of duplicates too\n",
    "    full =full.drop_duplicates(['UID'],keep='first') \n",
    "\n",
    "\n",
    "    to_drop = ['missing', 'dup_same_city',\n",
    "               'OwnerName',  'hash',\n",
    "               # 'Unique_Cluster', # can't drop until the end (shortest way to make files for each indiv cluster!!)\n",
    "               'City_y', 'State_y', 'Zip_y', 'Latitude_y', 'Longitude_y',]\n",
    "\n",
    "    full = full.drop(to_drop, axis = 1)\n",
    "    full.columns = full.columns.str.replace('_x', '').tolist()\n",
    "    full['DateOnMarket'] = full['DateOnMarket'].str.split('T').str[0]\n",
    "    full['DateOffMarket'] = full['DateOffMarket'].str.split('T').str[0]\n",
    "    full['DateOffMarket'] = full['DateOffMarket'].str.split('T').str[0]\n",
    "    full['DateOfObservation'] = [[x.split('T')[0] for x in l] for l in full['DateOfObservation'].tolist()]\n",
    "    cluster_to_json(full, master_list, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c85f1f98-afc9-4749-b3a3-c820eaffc892",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full refers to the full data set with UID and beds, baths, etc.\n",
    "def cluster_to_json(full, master_list,state):\n",
    "    clusters = master_list['Unique_Cluster'].tolist()\n",
    "    clusters = [x for x in clusters if state in x]\n",
    "\n",
    "    #clus = 'TX_4_0'\n",
    "    for clus in clusters:\n",
    "        # NON-SITE\n",
    "        # non-site JSON converting ---------------------------\n",
    "        # get list of rents, owners, etc for each home within this cluster\n",
    "        clus_df = full.loc[full['Unique_Cluster'] == clus]\n",
    "\n",
    "        # remove null values\n",
    "        for index, row in clus_df.iterrows():\n",
    "            if type(row['Rent']) == list:\n",
    "                tdf = pd.DataFrame(list(zip(row['Rent'], row['DateOfObservation'])),\n",
    "                           columns = ['rent', 'obsdates'])\n",
    "                tdf = tdf.dropna()\n",
    "                clus_df.at[index, 'DateOfObservation'] = tdf['obsdates'].tolist()\n",
    "                clus_df.at[index, 'Rent'] = tdf['rent'].tolist()\n",
    "\n",
    "\n",
    "        # df to json\n",
    "        out = clus_df.drop(['Unique_Cluster'], axis = 1).to_json(orient='records')[1:-1]\n",
    "\n",
    "        # save json\n",
    "        os.chdir(r'T:\\Single Family\\SFR\\02. DATABASE\\JSON\\B2R JSON Clusters') # save files in current directory\n",
    "        with open(clus + '__B2R_cluster_non-site.json', 'w') as f:\n",
    "            f.write(\"[\")\n",
    "            f.write(out)\n",
    "            f.write(\"]\")\n",
    "        # ---------------------------------------------------\n",
    "\n",
    "        # SITE\n",
    "        site_clus_df = clus_df\n",
    "        site_clus_df['Count'] = 1\n",
    "\n",
    "        # ----------------------------\n",
    "        # restructure\n",
    "        site_clus_df['Bed'] = np.empty((len(site_clus_df), 0)).tolist()\n",
    "        site_clus_df['Baths'] = np.empty((len(site_clus_df), 0)).tolist()\n",
    "        site_clus_df['SQFT'] = np.empty((len(site_clus_df), 0)).tolist()\n",
    "\n",
    "        for index, x in site_clus_df.iterrows(): #x is row\n",
    "            # concatenate name description and set it to new column\n",
    "            beds = x['Characteristics'].get('Beds')\n",
    "            baths = x['Characteristics'].get('Baths')\n",
    "            sqft = x['Characteristics'].get('SQFT')\n",
    "\n",
    "            site_clus_df.at[index, 'Name'] = str(beds) + ' Bd ' + str(baths) + ' Bath ' + str(sqft) + ' Sqft'\n",
    "\n",
    "            site_clus_df.at[index, 'Bed'] = beds\n",
    "            site_clus_df.at[index, 'Baths'] = baths\n",
    "            site_clus_df.at[index, 'SQFT'] = sqft\n",
    "\n",
    "            # prepare other columns for future list concatenation\n",
    "            if type(x['Rent']) != list:\n",
    "                site_clus_df.at[index, 'Rent'] = [x['Rent']]\n",
    "            if type(x['DateOfObservation']) != list:\n",
    "                site_clus_df.at[index, 'DateOfObservation'] = [x['DateOfObservation']]\n",
    "        # ----------------------------\n",
    "\n",
    "        site_clus_df1 = site_clus_df.groupby(['Name'], as_index = False).agg({'Count': ['sum'], \n",
    "                                                                                'Rent': ['sum'], \n",
    "                                                                                'DateOfObservation': ['sum'],\n",
    "                                                                              'Bed': ['first'],\n",
    "                                                                              'Baths': ['first'],\n",
    "                                                                              'SQFT': ['first']\n",
    "                                                                                })\n",
    "        site_clus_df1.columns = site_clus_df1.columns.droplevel(1)\n",
    "        # ----------------------------\n",
    "\n",
    "        for index, row in site_clus_df1.iterrows():\n",
    "            tdf = pd.DataFrame(list(zip(row['Rent'], row['DateOfObservation'])),\n",
    "                       columns = ['rent', 'obsdates'])\n",
    "            #tdf = tdf.dropna()\n",
    "            mon_avg = tdf.groupby(pd.PeriodIndex(tdf['obsdates'], freq=\"M\"))['rent'].mean()\n",
    "            site_clus_df1.at[index, 'DateOfObservation'] = [str(x) for x in mon_avg.index]\n",
    "            site_clus_df1.at[index, 'Rent'] = [round(x,2) for x in list(mon_avg)]\n",
    "        # ----------------------------\n",
    "\n",
    "        site_clus_df1 = site_clus_df1.rename({'Count': 'CurrentObservedHouses'}, axis =1)\n",
    "\n",
    "\n",
    "        # df to json\n",
    "        out = site_clus_df1.to_json(orient='records')[1:-1] \n",
    "\n",
    "        # save json\n",
    "        with open(clus + '__B2R_cluster_site_plan.json', 'w') as f:\n",
    "            #f.write(\"[\\\"ClusterID\\\": \" + str(clus))\n",
    "            #f.write(\"\\\"Plan\\\": [\")\n",
    "            f.write(\"[\")\n",
    "            f.write(out)\n",
    "            f.write(\"]\")\n",
    "            #f.write(\"]\")\n",
    "        # ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4f05fe-16bb-4d1a-858b-ce4c8a6fbdf1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dir_files = ['FL_sfr.json']\n",
    "#state_file_name = dir_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ede1e-3c62-4ce0-b356-be68a7fb2dc8",
   "metadata": {},
   "source": [
    "# Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d413e-cc4d-4778-8ee8-fe99c4551ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90213d-2a13-4c0b-8db3-193288365a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#site_clus_df.assign(Name = lambda x: (str(x['Characteristics'].values[0].get('Beds')) + ' Bd' + str(x['Characteristics'].values[0].get('Baths')) + ' Bath' + str(x['Characteristics'].values[0].get('SQFT')) + ' Sqft'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb6ecf-8e12-41cd-b511-b0be4b21dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want uid, uncomment this \n",
    "# reference: https://stackoverflow.com/questions/23794082/pandas-groupby-and-join-lists\n",
    "# (make sure to repeat uid for each item in list)\n",
    "#site_clus_df1 = site_clus_df.groupby(['Beds', 'Baths', 'Sqft'], as_index = False).agg({'Count': ['sum'], 'Rent': ['sum'], 'DateofObservation': ['sum'], 'UID': lambda x: ' '.join(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76cf5b6-2cf8-469c-800f-9e46cbcdd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "site_clus_df1['Name'] = site_clus_df1['Beds'].astype(str)  + ' Bed '+ site_clus_df1['Baths'].astype(str) + ' Bath ' + site_clus_df1['Sqft'].astype(str) + ' Sqft'\n",
    "first_column = site_clus_df1.pop('Name')\n",
    "site_clus_df1.insert(0, 'Name', first_column)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
